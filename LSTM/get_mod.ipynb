{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ticker_names = [\"MSFT\", \"AAPL\", \"NVDA\", \"AMZN\", \"GOOG\", \\\n",
    "           \"GOOGL\", \"META\", \"TSM\", \"LLY\", \"AVGO\", \\\n",
    "            \"NVO\", \"TSLA\", \"JPM\", \"V\", \"WMT\", \\\n",
    "                \"UNH\", \"MA\", \"XOM\", \"ASML\", \"JNJ\"]\n",
    "dfs = []\n",
    "X = [] # 3d arr: num_ticker * num_sample * num_features\n",
    "Y = [] # 2d arr: num_ticker * num_sample\n",
    "\n",
    "for name in ticker_names:  \n",
    "    df = pd.read_csv(f'../DATA/Cleaned_Datas/cleaned_{name}_prices.csv')\n",
    "    dfs.append(df)\n",
    "    X.append(df.iloc[:, 1:4].values)\n",
    "    Y.append(df.iloc[:, 4].values)\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "X_trains = [] # 3d arr: ticker * sample * features\n",
    "Y_trains = [] # 2d arr: ticker * sample\n",
    "X_cvs = [] # 3d arr: ticker * sample * features\n",
    "Y_cvs = [] # 2d arr: ticker * sample\n",
    "X_tests = [] # 3d arr: ticker * sample * features\n",
    "Y_tests = [] # 2d arr: ticker * sample\n",
    "\n",
    "training_sizes = []\n",
    "cv_sizes = []\n",
    "test_sizes = []\n",
    "\n",
    "for ticker in X:\n",
    "    train_size = int(len(ticker) * 0.6)\n",
    "    cv_size = int(len(ticker) * 0.2)\n",
    "    test_size = len(ticker) - train_size - cv_size\n",
    "\n",
    "    training_sizes.append(train_size)\n",
    "    cv_sizes.append(cv_size)\n",
    "    test_sizes.append(test_size)\n",
    "\n",
    "    X_train = ticker[:train_size]\n",
    "    X_cv = ticker[train_size:train_size+cv_size]\n",
    "    X_test = ticker[train_size+cv_size:]\n",
    "\n",
    "    X_trains.append(X_train)\n",
    "    X_cvs.append(X_cv)\n",
    "    X_tests.append(X_test)\n",
    "\n",
    "\n",
    "for ticker in Y:\n",
    "    train_size = int(len(ticker) * 0.6)\n",
    "    cv_size = int(len(ticker) * 0.2)\n",
    "\n",
    "    Y_train = ticker[:train_size]\n",
    "    Y_cv = ticker[train_size:train_size+cv_size]\n",
    "    Y_test = ticker[train_size+cv_size:]\n",
    "\n",
    "    Y_trains.append(Y_train)\n",
    "    Y_cvs.append(Y_cv)\n",
    "    Y_tests.append(Y_test)\n",
    "\n",
    "\n",
    "X_trains = np.array(X_trains) # 3d arr: ticker * sample * features\n",
    "Y_trains = np.array(Y_trains) # 2d arr: ticker * sample\n",
    "X_cvs = np.array(X_cvs) # 3d arr: ticker * sample * features\n",
    "Y_cvs = np.array(Y_cvs) # 2d arr: ticker * sample\n",
    "X_tests = np.array(X_tests) # 3d arr: ticker * sample * features\n",
    "Y_tests = np.array(Y_tests) # 2d arr: ticker * sample\n",
    "\n",
    "print(Y_trains[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Dropout\n",
    "from tensorflow.keras.losses import mse, mae\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_steps = 7 # a week of steps\n",
    "input_dim = 3 # OHL\n",
    "\n",
    "# # Initialize an empty list to store the reshaped sequences\n",
    "# X_train_reshaped = []\n",
    "\n",
    "# # Calculate the new number of samples based on the time steps\n",
    "# num_sequences = len(X_train) - time_steps + 1\n",
    "\n",
    "# # Reshape the data\n",
    "# for i in range(num_sequences):\n",
    "#     # Extract a sequence of length `time_steps` starting from index i\n",
    "#     sequence = X_train[i:i + time_steps, :]\n",
    "#     # Append the sequence to your new list\n",
    "#     X_train_reshaped.append(sequence)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(ticker_names)):\n",
    "    x_t = X_trains[i]\n",
    "    y_t = Y_trains[i]\n",
    "    x_cv = X_cvs[i]\n",
    "    y_cv = Y_cvs[i]\n",
    "\n",
    "    # print(x_t.shape, y_t.shape)\n",
    "\n",
    "    scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_x.fit(x_t)\n",
    "    # scaler_y.fit(y_t)\n",
    "    x_t_scaled = scaler_x.transform(x_t)\n",
    "    x_cv_scaled = scaler_x.transform(x_cv)\n",
    "    # y_t_scaled = scaler_y.transform(y_t)\n",
    "    # y_cv_scaled = scaler_y.transform(y_cv)\n",
    "\n",
    "    # sample_len = len(x_t) - time_steps + 1\n",
    "\n",
    "    # for j in range(sample_len):\n",
    "    #     sample_reshaped = []\n",
    "    #     for h in range(time_steps):\n",
    "    #         sample_len.append(X_trains[h+j])\n",
    "        \n",
    "    #     x_t_scaled[j] = sample_reshaped\n",
    "\n",
    "    # print(x_t.shape)\n",
    "\n",
    "    model = Sequential([   \n",
    "        LSTM(units= 256, activation='relu', return_sequences=True, input_shape=(time_steps, input_dim)),\n",
    "        Dropout(0.1),\n",
    "        LSTM(units= 128, activation='relu', return_sequences=False),\n",
    "        Dropout(0.1),\n",
    "        Dense(units= 64, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(units= 1, activation='linear'),\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "    model.fit(x_t_scaled, y_t_scaled, validation_data=(x_cv_scaled, y_cv_scaled) , epochs=100, batch_size=64)\n",
    "    model.save(f'{ticker_names[i]}.h5')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
